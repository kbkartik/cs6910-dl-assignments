{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment3.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "wJeaqNXITl4e",
        "3Kw6YmKjU0A6",
        "BrYPVPm8UGkh",
        "vvnL6AXJUMc4"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\n",
        "!tar -xf dakshina_dataset_v1.0.tar\n",
        "!pip -q install wandb\n",
        "!pip -q install tensorflow-addons"
      ],
      "metadata": {
        "id": "CbtleUBg63ww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rm -rf dakshina_dataset_v1.0.tar"
      ],
      "metadata": {
        "id": "btozvMsG652N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "import os\n",
        "os.environ[\"WANDB_SILENT\"] = \"true\"\n",
        "os.environ['WANDB_CONSOLE'] = 'off'\n",
        "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
        "wandb.login(key='')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDT28Gy167Oj",
        "outputId": "1066fe59-56e7-4cd3-fe3f-c2ede7037499"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from seq2seq import Seq2Seq\n",
        "from transliteration import Transliteration\n",
        "from hyperparams import default_q2_config, bayes_config_q2_instance, default_q5_config, bayes_config_q5_instance, best_q2_config, best_q5_config\n",
        "import wandb"
      ],
      "metadata": {
        "id": "9wSsuujoTpnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## W/o attention"
      ],
      "metadata": {
        "id": "wJeaqNXITl4e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list_runs = []\n",
        "\n",
        "def run_sweep():\n",
        "    wandb.init(config=default_q2_config)\n",
        "    config = wandb.config\n",
        "    HYPERPARAMS = config._as_dict()\n",
        "\n",
        "    # Set the run name\n",
        "    wandb.run.name = HYPERPARAMS[\"cell_type\"] + \"_emb_\" + str(HYPERPARAMS[\"emb_dim\"]) + \"_n_enc_layers_\" + str(HYPERPARAMS[\"n_enc_layers\"])\n",
        "    wandb.run.name += \"_hid_dim_\" + str(HYPERPARAMS[\"hid_st_dim\"]) + \"_n_dec_layers_\" + str(HYPERPARAMS[\"n_dec_layers\"])\n",
        "    wandb.run.name += \"_dout_\" + str(HYPERPARAMS[\"dropout\"])\n",
        "    wandb.run.name += \"_bw_\" + str(HYPERPARAMS[\"beam_width\"]) + \"_optim_\" + HYPERPARAMS[\"optim\"]\n",
        "    wandb.run.name += \"_ep_\" + str(HYPERPARAMS[\"epochs\"]) + \"_batch_\" + str(HYPERPARAMS[\"batch_size\"])\n",
        "    wandb.run.name += \"_att_\" if HYPERPARAMS[\"attention\"] else \"\"\n",
        "\n",
        "    if len(list_runs) == 0 or wandb.run.name not in list_runs:\n",
        "        print(wandb.run.name)\n",
        "        list_runs.append(wandb.run.name)\n",
        "        # Loading the datasets\n",
        "        transliteration = Transliteration(HYPERPARAMS, tgt_lang='hi')\n",
        "\n",
        "        # Training and evaluating the model\n",
        "        model = Seq2Seq(transliteration.dataset_configs, HYPERPARAMS)\n",
        "        model.train(transliteration.train_ds, transliteration.val_ds)\n",
        "    wandb.finish(quiet=True)\n",
        "    #model.evaluate(transliteration.test_ds, \"test\", write_to_file=True, make_plots=True)"
      ],
      "metadata": {
        "id": "_fEXxtO66e6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run the instance sweep\n",
        "sweep_id = wandb.sweep(bayes_config_q2_instance, project=\"test\", entity=\"kbdl\")\n",
        "wandb.agent(sweep_id, function=run_sweep, count=20)"
      ],
      "metadata": {
        "id": "Eh_1UT1g7RRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Best sweep"
      ],
      "metadata": {
        "id": "3Kw6YmKjU0A6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_best_sweep():\n",
        "    wandb.init(config=best_q2_config)\n",
        "    config = wandb.config\n",
        "    HYPERPARAMS = best_q2_config #config._as_dict()\n",
        "    \n",
        "    # Set the run name\n",
        "    wandb.run.name = HYPERPARAMS[\"cell_type\"] + \"_emb_\" + str(HYPERPARAMS[\"emb_dim\"]) + \"_n_enc_layers_\" + str(HYPERPARAMS[\"n_enc_layers\"])\n",
        "    wandb.run.name += \"_hid_dim_\" + str(HYPERPARAMS[\"hid_st_dim\"]) + \"_n_dec_layers_\" + str(HYPERPARAMS[\"n_dec_layers\"])\n",
        "    wandb.run.name += \"_dout_\" + str(HYPERPARAMS[\"dropout\"])\n",
        "    wandb.run.name += \"_bw_\" + str(HYPERPARAMS[\"beam_width\"]) + \"_optim_\" + HYPERPARAMS[\"optim\"]\n",
        "    wandb.run.name += \"_ep_\" + str(HYPERPARAMS[\"epochs\"]) + \"_batch_\" + str(HYPERPARAMS[\"batch_size\"])\n",
        "    wandb.run.name += \"_att_\" if HYPERPARAMS[\"attention\"] else \"\"\n",
        "\n",
        "    print(wandb.run.name)\n",
        "    wandb.run.name = 'best_sweep_wo_attn'\n",
        "    # Loading the datasets\n",
        "    transliteration = Transliteration(HYPERPARAMS, tgt_lang='hi')\n",
        "\n",
        "    # Training and evaluating the model\n",
        "    model = Seq2Seq(transliteration.dataset_configs, HYPERPARAMS)\n",
        "    model.train(transliteration.train_ds, transliteration.val_ds, eval_on_val_ds=False)\n",
        "    model.evaluate(transliteration.test_ds, \"test\", write_to_file=True, make_plots=True)\n",
        "    wandb.finish(quiet=True)"
      ],
      "metadata": {
        "id": "D5LSZj6ifsoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run the instance sweep\n",
        "sweep_id = wandb.sweep(best_q2_instance, project=\"test\", entity=\"kbdl\")\n",
        "wandb.agent(sweep_id, function=run_best_sweep, count=1)"
      ],
      "metadata": {
        "id": "kgSSpSSjf09I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## With attention"
      ],
      "metadata": {
        "id": "BrYPVPm8UGkh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list_runs = []\n",
        "\n",
        "def run_sweep():\n",
        "    wandb.init(config=default_q5_config)\n",
        "    config = wandb.config\n",
        "    HYPERPARAMS = config._as_dict()\n",
        "\n",
        "    # Set the run name\n",
        "    wandb.run.name = HYPERPARAMS[\"cell_type\"] + \"_emb_\" + str(HYPERPARAMS[\"emb_dim\"]) + \"_n_enc_layers_\" + str(HYPERPARAMS[\"n_enc_layers\"])\n",
        "    wandb.run.name += \"_hid_dim_\" + str(HYPERPARAMS[\"hid_st_dim\"]) + \"_n_dec_layers_\" + str(HYPERPARAMS[\"n_dec_layers\"])\n",
        "    wandb.run.name += \"_dout_\" + str(HYPERPARAMS[\"dropout\"])\n",
        "    wandb.run.name += \"_bw_\" + str(HYPERPARAMS[\"beam_width\"]) + \"_optim_\" + HYPERPARAMS[\"optim\"]\n",
        "    wandb.run.name += \"_ep_\" + str(HYPERPARAMS[\"epochs\"]) + \"_batch_\" + str(HYPERPARAMS[\"batch_size\"])\n",
        "    wandb.run.name += \"_att_\" if HYPERPARAMS[\"attention\"] else \"\"\n",
        "\n",
        "    if len(list_runs) == 0 or wandb.run.name not in list_runs:\n",
        "        print(wandb.run.name)\n",
        "        list_runs.append(wandb.run.name)\n",
        "        # Loading the datasets\n",
        "        transliteration = Transliteration(HYPERPARAMS, tgt_lang='hi')\n",
        "\n",
        "        # Training and evaluating the model\n",
        "        model = Seq2Seq(transliteration.dataset_configs, HYPERPARAMS)\n",
        "        model.train(transliteration.train_ds, transliteration.val_ds)\n",
        "    wandb.finish(quiet=True)\n",
        "    #model.evaluate(transliteration.test_ds, \"test\", write_to_file=True, make_plots=True)"
      ],
      "metadata": {
        "id": "qJaTKvKoUN9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run the instance sweep\n",
        "sweep_id = wandb.sweep(bayes_config_q5_instance, project=\"test\", entity=\"kbdl\")\n",
        "wandb.agent(sweep_id, function=run_sweep, count=20)"
      ],
      "metadata": {
        "id": "kAYNRickUQwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Best sweep"
      ],
      "metadata": {
        "id": "vvnL6AXJUMc4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_best_sweep():\n",
        "    wandb.init(config=best_q5_config)\n",
        "    config = wandb.config\n",
        "    HYPERPARAMS = best_q5_config #config._as_dict()\n",
        "    \n",
        "    # Set the run name\n",
        "    wandb.run.name = HYPERPARAMS[\"cell_type\"] + \"_emb_\" + str(HYPERPARAMS[\"emb_dim\"]) + \"_n_enc_layers_\" + str(HYPERPARAMS[\"n_enc_layers\"])\n",
        "    wandb.run.name += \"_hid_dim_\" + str(HYPERPARAMS[\"hid_st_dim\"]) + \"_n_dec_layers_\" + str(HYPERPARAMS[\"n_dec_layers\"])\n",
        "    wandb.run.name += \"_dout_\" + str(HYPERPARAMS[\"dropout\"])\n",
        "    wandb.run.name += \"_bw_\" + str(HYPERPARAMS[\"beam_width\"]) + \"_optim_\" + HYPERPARAMS[\"optim\"]\n",
        "    wandb.run.name += \"_ep_\" + str(HYPERPARAMS[\"epochs\"]) + \"_batch_\" + str(HYPERPARAMS[\"batch_size\"])\n",
        "    wandb.run.name += \"_att_\" if HYPERPARAMS[\"attention\"] else \"\"\n",
        "\n",
        "    print(wandb.run.name)\n",
        "    wandb.run.name = 'best_attention_sweep'\n",
        "    # Loading the datasets\n",
        "    transliteration = Transliteration(HYPERPARAMS, tgt_lang='hi')\n",
        "\n",
        "    # Training and evaluating the model\n",
        "    model = Seq2Seq(transliteration.dataset_configs, HYPERPARAMS)\n",
        "    model.train(transliteration.train_ds, transliteration.val_ds, eval_on_val_ds=False)\n",
        "    model.evaluate(transliteration.test_ds, \"test\", write_to_file=True, make_plots=True)\n",
        "    wandb.finish(quiet=True)"
      ],
      "metadata": {
        "id": "qJUK2u23uSO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run the instance sweep\n",
        "sweep_id = wandb.sweep(best_q5_instance, project=\"test\", entity=\"kbdl\")\n",
        "wandb.agent(sweep_id, function=run_best_sweep, count=1)"
      ],
      "metadata": {
        "id": "RAA0bVocAd6X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}