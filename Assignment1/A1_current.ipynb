{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A1_current.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip --quiet install wandb\n",
        "import wandb\n",
        "import os\n",
        "#os.environ[\"WANDB_SILENT\"] = \"false\"\n",
        "wandb.login()"
      ],
      "metadata": {
        "id": "lFWY1xRwh6JT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJWRpx_1-Omq"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import fashion_mnist\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from activations import activation_dict, Softmax\n",
        "from utils import preprocess\n",
        "from layers import Linear\n",
        "from loss_function import Categorical_CE\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "x_train, x_test, y_train, y_test = preprocess(x_train, x_test, y_train, y_test, preprocess_type='normalize')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Backprop:\n",
        "\n",
        "    def backward(self, loss_fn, layer_activation, layer_wise_output, network, y_true):\n",
        "\n",
        "        assert len(layer_activation) == len(network), \"We've an issue\"\n",
        "        \n",
        "        layer_gradients = []\n",
        "        accum_grad = loss_fn.gradient(layer_wise_output[-1], y_true)\n",
        "        for i in range(len(network))[::-1]:\n",
        "            if i == len(network) - 1:\n",
        "                accum_grad = np.multiply(accum_grad, layer_activation[i].gradient(layer_wise_output[i+1], y_true))\n",
        "            else:\n",
        "                accum_grad = np.multiply(accum_grad, layer_activation[i].gradient(layer_wise_output[i+1]))\n",
        "            grad_w = np.dot(layer_wise_output[i].T, accum_grad)\n",
        "            accum_grad = np.dot(accum_grad, network[i].weights.T)\n",
        "            \n",
        "            # Append gradients\n",
        "            layer_gradients.append(grad_w)\n",
        "        \n",
        "        layer_gradients.reverse()\n",
        "        \n",
        "        return layer_gradients\n",
        "\n",
        "class fNN:\n",
        "\n",
        "    def __init__(self, input_dims, n_classes, activation, batch_size, num_epochs, n_hidden_neurons, \n",
        "                 lr, n_hidden_layers, weight_init_type):\n",
        "        \n",
        "        self.num_epochs = num_epochs\n",
        "        self.n_hidden_layers = n_hidden_layers\n",
        "        self.lr = lr\n",
        "        self.batch_size = batch_size\n",
        "        self.activation_dict = activation_dict\n",
        "\n",
        "        self.NN = []\n",
        "        self.activations = []\n",
        "\n",
        "        self.activation_fn, gain = self.activation_dict[activation] #Sigmoid()\n",
        "        self.softmax_activation = Softmax()\n",
        "        self.loss_fn = Categorical_CE()\n",
        "        self.backprop = Backprop()\n",
        "\n",
        "        for l in range(self.n_hidden_layers+1):\n",
        "            fc = None\n",
        "            if l == 0:\n",
        "                fc = Linear(input_dims, n_hidden_neurons[l]+1, weight_init_type, gain)\n",
        "                self.activations.append(self.activation_fn)\n",
        "            elif l == self.n_hidden_layers:\n",
        "                fc = Linear(n_hidden_neurons[l-1]+1, n_classes, weight_init_type, gain)\n",
        "                self.activations.append(self.softmax_activation)\n",
        "            else:\n",
        "                fc = Linear(n_hidden_neurons[l-1]+1, n_hidden_neurons[l]+1, weight_init_type, gain)\n",
        "                self.activations.append(self.activation_fn)\n",
        "\n",
        "            self.NN.append(fc)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        layer_wise_output = [x]\n",
        "\n",
        "        for i in range(self.n_hidden_layers+1):\n",
        "            x = self.NN[i](x)\n",
        "            x = self.activations[i](x)\n",
        "            layer_wise_output.append(x)\n",
        "        \n",
        "        return layer_wise_output\n",
        "\n",
        "    def fit(self, x_train, y_train):\n",
        "        \n",
        "        n_data = x_train.shape[0]\n",
        "\n",
        "        for ep in  tqdm(range(self.num_epochs)):\n",
        "            d = 0\n",
        "            while d < n_data:\n",
        "                t = min(n_data - d, self.batch_size)\n",
        "            \n",
        "                x_mini_batch = x_train[d:d+t]\n",
        "                y_mini_batch = y_train[d:d+t]\n",
        "\n",
        "                layer_wise_output = self.forward(x_mini_batch)\n",
        "                loss = self.loss_fn.loss(layer_wise_output[-1], y_mini_batch)\n",
        "                layer_gradients = self.backprop.backward(self.loss_fn, self.activations, layer_wise_output, self.NN, y_mini_batch)\n",
        "\n",
        "                for i in range(self.n_hidden_layers+1):\n",
        "                    self.NN[i].weights -= self.lr * layer_gradients[i]\n",
        "\n",
        "                layer_gradients = []\n",
        "                d += self.batch_size\n",
        "\n",
        "        \"\"\"\n",
        "        num_itrs = int(float(n_data/self.batch_size)*self.num_epochs)\n",
        "        d = 0\n",
        "        t = batch_size\n",
        "\n",
        "        for n in tqdm(range(num_itrs)):\n",
        "            if d + batch_size > n_data:\n",
        "                d = 0\n",
        "                t = min(n_data - self.batch_size, self.batch_size)\n",
        "            \n",
        "            x_mini_batch = x_train[d:d+t]\n",
        "            y_mini_batch = y_train[d:d+t]\n",
        "\n",
        "            layer_wise_output = self.forward(x_mini_batch)\n",
        "            layer_gradients = self.backprop.backward(self.loss_fn, self.activations, layer_wise_output, self.NN, y_mini_batch)\n",
        "\n",
        "            for i in range(self.n_hidden_layers+1):\n",
        "                self.NN[i].weights -= lr * layer_gradients[i]\n",
        "\n",
        "            layer_gradients = []\n",
        "            d += batch_size\n",
        "        \"\"\"\n",
        "\n",
        "    def evaluate(self, x_test, y_test):\n",
        "\n",
        "        num_test_datapoints = x_test.shape[0]\n",
        "        layer_wise_outputs = self.forward(x_test)\n",
        "        y_hat = layer_wise_outputs[-1]\n",
        "        y_pred = np.argmax(y_hat, axis=1)\n",
        "        accuracy = (len(np.argwhere(y_pred == y_test))/num_test_datapoints)*100\n",
        "\n",
        "        return accuracy"
      ],
      "metadata": {
        "id": "N5id1z-b-ztE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_dims = x_train.shape[1]\n",
        "n_hidden_layers = 5\n",
        "n_hidden_neurons = [32, 64, 128, 64, 32] # This will be constant across all layers\n",
        "batch_size = 64\n",
        "lr = 0.001\n",
        "num_epochs = 10\n",
        "n_classes = 10\n",
        "weight_init_type = 'random'\n",
        "\n",
        "model = fNN(input_dims, n_classes, \"sigmoid\", batch_size, num_epochs, n_hidden_neurons, lr, n_hidden_layers, weight_init_type)\n",
        "model.fit(x_train, y_train)\n",
        "print(\"Accuracy: \", model.evaluate(x_test, y_test))"
      ],
      "metadata": {
        "id": "nwifA8g2C9SB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_config = {\n",
        "    \"method\" : \"grid\",\n",
        "    \"metric\" : {\n",
        "        \"name\" : \"acc\",\n",
        "        \"goal\" : \"maximize\",\n",
        "    },\n",
        "    \"parameters\" : {\n",
        "        \"epochs\" : {\n",
        "            \"values\" : [5, 10]\n",
        "        },\n",
        "        \"n_hidden_layers\" : {\n",
        "            \"values\" : [3, 4, 5]\n",
        "        },\n",
        "        \"hidden_layer_size\": {\n",
        "            \"values\" : [32, 64, 128]\n",
        "        },\n",
        "        \"learning_rate\" : {\n",
        "            \"values\" : [0.0001, 0.001]\n",
        "        },\n",
        "        \"batch_size\" : {\n",
        "            \"values\" : [16, 32, 64]\n",
        "        },\n",
        "        \"weight_init_type\" : {\n",
        "            \"values\" : ['random', 'xavier']\n",
        "        },\n",
        "        \"activation\" : {\n",
        "            \"values\" : ['sigmoid']#, 'tanh', 'relu']\n",
        "        },\n",
        "    }\n",
        "}\n",
        "\n",
        "default_config = {\n",
        "    \"activation\" : 'sigmoid',\n",
        "    \"batch_size\" : 16,\n",
        "    \"epochs\" : 5,\n",
        "    \"hidden_layer_size\": 32,\n",
        "    \"learning_rate\" : 0.0001,\n",
        "    \"n_hidden_layers\" : 3,\n",
        "    \"weight_init_type\" : 'random',\n",
        "}\n",
        "\n",
        "def train():\n",
        "    wandb.init(config=default_config)\n",
        "    config = wandb.config\n",
        "    n_classes = 10\n",
        "    input_dims = x_train.shape[1]\n",
        "    model = fNN(input_dims, n_classes, *list(config._as_dict().values())[:-1])\n",
        "    model.fit(x_train, y_train)  # your model training code here\n",
        "    accuracy = model.evaluate(x_test, y_test)\n",
        "    wandb.log({'acc': accuracy})\n",
        "    wandb.finish(quiet=True)\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"test\", entity=\"kbdl\")\n",
        "wandb.agent(sweep_id, function=train)\n",
        "#wandb.finish()"
      ],
      "metadata": {
        "id": "mRb1dzwH264j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "trial_y_true = np.array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0])\n",
        "trial_y_true = trial_y_true.reshape(1, len(trial_y_true))\n",
        "trial_y_pred = np.array([0, 0.03, 0.01, 0.6, 0, 0.1, 0.04, 0, 0.22, 0])\n",
        "trial_y_pred = trial_y_pred.reshape(1, len(trial_y_pred))\n",
        "lay = [0, trial_y_pred]\n",
        "nn = [1]\n",
        "model = fNN(input_dims, n_classes, n_hidden_layers, n_hidden_neurons, weight_init)\n",
        "a, b, c = model.backprop.backward(model.loss_fn, [model.softmax_activation], lay, nn, trial_y_true)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "_bC6wLQTRkkb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}