{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A1_current.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "!pip --quiet install wandb\n",
        "import wandb\n",
        "import os\n",
        "#os.environ[\"WANDB_SILENT\"] = \"false\"\n",
        "wandb.login()\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "lFWY1xRwh6JT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "11cd906a-5671-46c2-e4be-4ef8d2a6377b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n!pip --quiet install wandb\\nimport wandb\\nimport os\\n#os.environ[\"WANDB_SILENT\"] = \"false\"\\nwandb.login()\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "SJWRpx_1-Omq"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import fashion_mnist\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from activations import activation_dict, Softmax\n",
        "from utils import preprocess\n",
        "from layers import Linear\n",
        "from loss_function import Categorical_CE\n",
        "from backprop import Backprop\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "x_train, x_test, y_train, y_test = preprocess(x_train, x_test, y_train, y_test, preprocess_type='normalize')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SGD:\n",
        "\n",
        "    def __init__(self, lr):\n",
        "        self.lr = lr\n",
        "\n",
        "    def step(self, network, grads):\n",
        "        \n",
        "        n_layer_weights = len(network)\n",
        "        for i in range(n_layer_weights):\n",
        "            network[i].weights -= self.lr * grads[i]\n",
        "        \n",
        "        return network\n",
        "\n",
        "class SGDMomentum:\n",
        "\n",
        "    def __init__(self, lr, mu):\n",
        "        self.lr = lr\n",
        "        self.mu = mu\n",
        "        self.history = []\n",
        "\n",
        "    def step(self, network, grads):\n",
        "\n",
        "        n_layer_weights = len(network)\n",
        "        for i in range(n_layer_weights):\n",
        "            t = self.lr * grads[i]\n",
        "            if len(self.history) != n_layer_weights:\n",
        "                # Same as initializing history to 0\n",
        "                self.history.append(t)\n",
        "            else:\n",
        "                self.history[i] = self.mu * self.history[i] + t\n",
        "\n",
        "            # Updating layer weights\n",
        "            network[i].weights -= self.history[i]\n",
        "        \n",
        "        return network\n",
        "\n",
        "class RMSProp:\n",
        "\n",
        "    def __init__(self, lr, beta):\n",
        "        self.lr = lr\n",
        "        self.beta = beta\n",
        "        self.epsilon = 1e-6\n",
        "        self.history = []\n",
        "        \n",
        "    def step(self, network, grads):\n",
        "\n",
        "        n_layer_weights = len(network)\n",
        "        for i in range(n_layer_weights):\n",
        "            t = (1 - self.beta) * np.power(grads[i], 2)\n",
        "            if len(self.history) != n_layer_weights:\n",
        "                # Same as initializing history to 0\n",
        "                self.history.append(t)\n",
        "            else:\n",
        "                self.history[i] = self.beta * self.history[i] + t\n",
        "\n",
        "            per_weight_hist = np.sqrt(self.history[i] + self.epsilon)\n",
        "            # Updating layer weights\n",
        "            network[i].weights -= self.lr * np.divide(grads[i], per_weight_hist)\n",
        "    \n",
        "        return network"
      ],
      "metadata": {
        "id": "zo7jLKKM1v-1"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class fNN:\n",
        "\n",
        "    def __init__(self, input_dims, n_classes, activation, batch_size, num_epochs, n_hidden_neurons, \n",
        "                 lr, n_hidden_layers, weight_init_type):\n",
        "        \n",
        "        self.NN = []\n",
        "        self.activations = []\n",
        "                \n",
        "        self.num_epochs = num_epochs\n",
        "        self.n_hidden_layers = n_hidden_layers\n",
        "        self.lr = lr\n",
        "        self.batch_size = batch_size\n",
        "        self.activation_dict = activation_dict\n",
        "\n",
        "        self.activation_fn, gain = self.activation_dict[activation]\n",
        "        self.softmax_activation = Softmax()\n",
        "        self.loss_fn = Categorical_CE()\n",
        "        self.backprop = Backprop()\n",
        "\n",
        "        optim_hyperparams = 0\n",
        "        self.create_model(weight_init_type, gain)\n",
        "\n",
        "        #self.optimizer = SGD(lr)\n",
        "        #self.optimizer = SGDMomentum(lr, 0.9)\n",
        "        self.optimizer = RMSProp(lr, 0.9)\n",
        "\n",
        "    def create_model(self, weight_init_type, gain):\n",
        "\n",
        "        for l in range(self.n_hidden_layers+1):\n",
        "            fc = None\n",
        "            if l == 0:\n",
        "                fc = Linear(input_dims, n_hidden_neurons[l]+1, weight_init_type, gain)\n",
        "                self.activations.append(self.activation_fn)\n",
        "            elif l == self.n_hidden_layers:\n",
        "                fc = Linear(n_hidden_neurons[l-1]+1, n_classes, weight_init_type, gain)\n",
        "                self.activations.append(self.softmax_activation)\n",
        "            else:\n",
        "                fc = Linear(n_hidden_neurons[l-1]+1, n_hidden_neurons[l]+1, weight_init_type, gain)\n",
        "                self.activations.append(self.activation_fn)\n",
        "\n",
        "            self.NN.append(fc)\n",
        "        \n",
        "    def forward(self, x):\n",
        "\n",
        "        layer_wise_output = [x]\n",
        "        for i in range(self.n_hidden_layers+1):\n",
        "            x = self.NN[i](x)\n",
        "            x = self.activations[i](x)\n",
        "            layer_wise_output.append(x)\n",
        "        \n",
        "        return layer_wise_output\n",
        "\n",
        "    def fit(self, x_train, y_train):\n",
        "        \n",
        "        n_data = x_train.shape[0]\n",
        "\n",
        "        for ep in  tqdm(range(self.num_epochs)):\n",
        "            d = 0\n",
        "            while d < n_data:\n",
        "                t = min(n_data - d, self.batch_size)\n",
        "            \n",
        "                x_mini_batch = x_train[d:d+t]\n",
        "                y_mini_batch = y_train[d:d+t]\n",
        "\n",
        "                layer_wise_output = self.forward(x_mini_batch)\n",
        "                loss = self.loss_fn.loss(layer_wise_output[-1], y_mini_batch)\n",
        "                layer_gradients = self.backprop.backward(self.loss_fn, self.activations, layer_wise_output, self.NN, y_mini_batch)\n",
        "\n",
        "                self.NN = self.optimizer.step(self.NN, layer_gradients)\n",
        "\n",
        "                layer_gradients = []\n",
        "                d += self.batch_size\n",
        "\n",
        "    def evaluate(self, x_test, y_test):\n",
        "\n",
        "        num_test_datapoints = x_test.shape[0]\n",
        "        layer_wise_outputs = self.forward(x_test)\n",
        "        y_hat = layer_wise_outputs[-1]\n",
        "        y_pred = np.argmax(y_hat, axis=1)\n",
        "        accuracy = (len(np.argwhere(y_pred == y_test))/num_test_datapoints)*100\n",
        "\n",
        "        return accuracy"
      ],
      "metadata": {
        "id": "N5id1z-b-ztE"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_dims = x_train.shape[1]\n",
        "n_hidden_layers = 5\n",
        "n_hidden_neurons = [32, 64, 128, 64, 32] # This will be constant across all layers\n",
        "batch_size = 64\n",
        "lr = 0.0001\n",
        "num_epochs = 10\n",
        "n_classes = 10\n",
        "weight_init_type = 'random'\n",
        "\n",
        "model = fNN(input_dims, n_classes, \"relu\", batch_size, num_epochs, n_hidden_neurons, lr, n_hidden_layers, weight_init_type)\n",
        "model.fit(x_train, y_train)\n",
        "print(\"Accuracy: \", model.evaluate(x_test, y_test))"
      ],
      "metadata": {
        "id": "nwifA8g2C9SB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "145a6975-0b04-4b42-d347-ebebc9808f04"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:42<00:00,  4.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  63.71\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "sweep_config = {\n",
        "    \"method\" : \"grid\",\n",
        "    \"metric\" : {\n",
        "        \"name\" : \"acc\",\n",
        "        \"goal\" : \"maximize\",\n",
        "    },\n",
        "    \"parameters\" : {\n",
        "        \"epochs\" : {\n",
        "            \"values\" : [5, 10]\n",
        "        },\n",
        "        \"n_hidden_layers\" : {\n",
        "            \"values\" : [3, 4, 5]\n",
        "        },\n",
        "        \"hidden_layer_size\": {\n",
        "            \"values\" : [32, 64, 128]\n",
        "        },\n",
        "        \"learning_rate\" : {\n",
        "            \"values\" : [0.0001, 0.001]\n",
        "        },\n",
        "        \"batch_size\" : {\n",
        "            \"values\" : [16, 32, 64]\n",
        "        },\n",
        "        \"weight_init_type\" : {\n",
        "            \"values\" : ['random', 'xavier']\n",
        "        },\n",
        "        \"activation\" : {\n",
        "            \"values\" : ['sigmoid']#, 'tanh', 'relu']\n",
        "        },\n",
        "    }\n",
        "}\n",
        "\n",
        "default_config = {\n",
        "    \"activation\" : 'sigmoid',\n",
        "    \"batch_size\" : 16,\n",
        "    \"epochs\" : 5,\n",
        "    \"hidden_layer_size\": 32,\n",
        "    \"learning_rate\" : 0.0001,\n",
        "    \"n_hidden_layers\" : 3,\n",
        "    \"weight_init_type\" : 'random',\n",
        "}\n",
        "\n",
        "def train():\n",
        "    wandb.init(config=default_config)\n",
        "    config = wandb.config\n",
        "    n_classes = 10\n",
        "    input_dims = x_train.shape[1]\n",
        "    model = fNN(input_dims, n_classes, *list(config._as_dict().values())[:-1])\n",
        "    model.fit(x_train, y_train)  # your model training code here\n",
        "    accuracy = model.evaluate(x_test, y_test)\n",
        "    wandb.log({'acc': accuracy})\n",
        "    wandb.finish(quiet=True)\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"test\", entity=\"kbdl\")\n",
        "wandb.agent(sweep_id, function=train)\n",
        "#wandb.finish()\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "mRb1dzwH264j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "trial_y_true = np.array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0])\n",
        "trial_y_true = trial_y_true.reshape(1, len(trial_y_true))\n",
        "trial_y_pred = np.array([0, 0.03, 0.01, 0.6, 0, 0.1, 0.04, 0, 0.22, 0])\n",
        "trial_y_pred = trial_y_pred.reshape(1, len(trial_y_pred))\n",
        "lay = [0, trial_y_pred]\n",
        "nn = [1]\n",
        "model = fNN(input_dims, n_classes, n_hidden_layers, n_hidden_neurons, weight_init)\n",
        "a, b, c = model.backprop.backward(model.loss_fn, [model.softmax_activation], lay, nn, trial_y_true)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "_bC6wLQTRkkb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}